
# logger options
image_save_iter:  1_000       # How often do you want to save output images during training
image_display_iter: 100       # How often do you want to display output images during training
display_size: 15              # How many images do you want to display each time
snapshot_save_iter: 10_000    # How often do you want to save trained models
log_iter: 1                   # How often do you want to log the training stats

# optimization options
random_seed: 1
max_iter: 1_500_000           # maximum number of training iterations
batch_size: 3                 # batch size
weight_decay: 0.0001          # weight decay
beta1: 0.5                    # Adam parameter
beta2: 0.999                  # Adam parameter
init: kaiming                 # initialization [gaussian/kaiming/xavier/orthogonal]
lr: 0.0001                    # initial learning rate
lr_policy: step               # learning rate scheduler
step_size: 100_000            # how often to decay learning rate
gamma: 0.5                    # how much to decay learning rate

gan_w: 26                     # weight of adversarial loss
recon_x_w: 0                  # weight of image reconstruction loss
recon_s_w: 0                  # weight of style reconstruction loss
recon_c_w: 0                  # weight of content reconstruction loss
recon_x_cyc_w: 0              # weight of explicit style augmented cycle consistency loss
vgg_w: 0                      # weight of domain-invariant perceptual loss
council_w: 6                  # weight of Council loss
council_abs_w: 0              # weight of Council loss abs between images if you use this loss there should be no need to use the regular council_w as well (with this loss active there will be less or no multimodal output)
council_abs_gray_scale: False # if set to true the council loss will use the gray scale image not the rgb one
# focus_loss
mask_zero_or_one_w: 0         # weight of part of the focus loss. controls how much the mask should approach binery values (zeros or ones)
mask_total_w: 0               # weight of the 2nd part of the focus loss. controls how much the mask should be small
mask_tv_w: 0                  # add Total Variation to the mask loss
focus_loss:                   # more variables for focus_loss
  mask_zero_or_one_center: 0.5 # mask_zero_or_one loss = 1/(mask_zero_or_one_epsilon + sum(abs(mask_zero_or_one_center - mask)))
  mask_zero_or_one_epsilon: 0.01
  mask_small_use_abs: False
  mask_small_use_square: True
  focus_loss_start_at_iter: 2_000
  do_w_loss_matching_focus: False # Unstable

abs_beginning_end: 0          # weight of abs loss between the input image and the generated one
abs_beginning_end_less_by: 1  # decrees the weight abs loss by that amount
abs_beginning_end_minimume: 0 # the minimal value abs loss should reach and stay on

do_w_loss_matching: True      # match between the mean of the gan and the council loss (mean of the last "loss_matching_hist_size" sampels
loss_matching_hist_size: 100  # the history to do avereg on so the gan loss and the council loss will match

# model options
gen:
  dim: 64                     # number of filters in the bottommost layer
  mlp_dim: 256                # number of filters in MLP
  style_dim: 64               # length of style code
  do_my_style: False
  activ: relu                 # activation function [relu/lrelu/prelu/selu/tanh]
  n_downsample: 2             # number of downsampling layers in content encoder
  n_res: 5                    # number of residual blocks in content encoder/decoder
  pad_type: zero              # padding type [zero/reflect]
  useRandomDis: False         # each generator is compared against all discriminetors
  num_of_mask_dim_to_add: 3

dis:
  dim: 64                     # number of filters in the bottommost layer
  norm: none                  # normalization layer [none/bn/in/ln]
  activ: lrelu                # activation function [relu/lrelu/prelu/selu/tanh]
  n_layer: 4                  # number of layers in D
  gan_type: lsgan             # GAN loss [lsgan/nsgan] RelativisticAverageHingeGAN
  num_scales: 2               # number of scales
  pad_type: zero              # padding type [zero/reflect]
  useRandomGen: False         # each discriminetor is compared against all generator
  do_Dis_only_gray: False
  numberOf_dis_relative_iteration: 1

council:
  council_size: 4             # number of council members
  numberOfCouncil_dis_relative_iteration: 4 # number of council discriminator training iteration for each gen iteration
  flipOnOff: False            # whether to filp the council loss on and off
  flipOnOff_start_with: True
  flipOnOff_On_iteration: 45
  flipOnOff_Off_iteration: 5
  discriminetro_less_style_by: 0.7 # the coucil member generted domain will be compare to 0.7 of each of the others genertors output domain
  council_start_at_iter: 1_000  # start using the council loss for the specified iteration


# data options
do_a2b: False
do_b2a: True
input_dim_a: 3                # number of image channels [1/3]
input_dim_b: 3                # number of image channels [1/3]
num_workers: 8                # number of data loading threads
new_size: 128 # 256                 # first resize the shortest image side to this size
crop_image_height: 128 # 256        # random crop image of this height
crop_image_width: 128 # 256         # random crop image of this width

# dataset augmentation
do_HorizontalFlip: True
do_VerticalFlip: False

do_ColorJitter_A: False
do_ColorJitter_B: False
ColorJitter_hue: 0.1
ColorJitter_brightness: 0.25
ColorJitter_saturation: 0.25
ColorJitter_contrast: 0.25

do_RandomGrayscale: True
RandomGrayscale_P: 0.05

do_RandomRotation: False
RandomRotation_degree: 35

do_RandomAffine: False
RandomAffine_translate_h: 0.2
RandomAffine_translate_w: 0.2

do_RandomPerspective: False

do_RandomResizedCrop: False
RandomResizedCrop_scale_max: 2
RandomResizedCrop_scale_min: 0.5
RandomResizedCrop_ratio_max: 4. / 3.
RandomResizedCrop_ratio_min: 3. / 4.

data_root: ./datasets/selfie2anime   # dataset folder location

inbalenceDataSets:
  imbalance_sub_dataset: False # Create basic inblace between domains for instance, A/1-male with galsses, A/2-male without galsses, B/1-female with galsses, B/2-female without galsses)
  ratio_A_1_to_2: 0.5 # 0.x from A/1 and 1-0.x from A/2
  ratio_B_1_to_2: 0.5 # 0.x from B/1 and 1-0.x from B/2

# ────────────────────────────────────────────────
# to use inbalenceDataSets dataset in this format:
#   ├──datasets
#       └──DATASET_NAME
#           ├──testA
#             ├──1
#               ├──im1.png
#               └── ...
#             └──2
#               ├──im2.png
#               └── ...
#           ├──testB
#             ├──1
#               ├──im3.png
#               └── ...
#             └──2
#               ├──im4.png
#               └── ...
#           ├──trainA
#             ├──1
#               ├──im5.png
#               └── ...
#             └──2
#               ├──im6.png
#               └── ...
#           └──trainB
#             ├──1
#               ├──im7.png
#               └── ...
#             └──2
#               ├──im8.png
#               └── ...
# ────────────────────────────────────────────────

misc:
  start_tensor_board: True
  start_tensor_board port: 6006
  do_test_Fid: True
  test_Fid_iter: 1_000
  test_Fid_num_of_im: 100
  do_telegram_report: True  # set to true to have progress report using telegram. run pip install python-telegram-bot, install telegram app, run train.py and follow the instructions.      (you need to obtain 2 numbers bot_token and chat_id then insert them in to a yaml file that will be created once you run train.py)
  do_telegram_send_config_file: True
  telegram_report_add_prefix: selfie2anime_
